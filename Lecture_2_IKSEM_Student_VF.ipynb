{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44dc9fd8",
   "metadata": {},
   "source": [
    "---\n",
    "University Paris 1 Panthéon-Sorbonne \n",
    "\n",
    "Introduction to Machine Learning\n",
    "\n",
    "Dr. Nourhène BEN RABAH\n",
    "\n",
    "---\n",
    "\n",
    "# Lecture 2: Data scaling, data normalization and data transformation \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In the last session, we discovered the first stage in data preparation, which is data cleaning.  In this session you will learn about data normalization, data scaling and data transformation. In the context of data preparation, these methods are important  to make the data in a suitable format for the ML algorithms. \n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color:lightblue; padding:1px\">\n",
    "<strong>Let's note that Normalization and Scaling are often used as synonyms, with slightly different goals. </strong>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "Let's explore each of these methods: \n",
    "\n",
    "#### 1) Data Scaling \n",
    "\n",
    "It's very possible that our dataset contains attributes that are scalable (i.e. the attributes have a large difference in the scale).  We can't provide this data to the ML algorithm because the large difference in the scale values can cause problems when comparing or combining them. \n",
    "\n",
    "Scaling the data makes sure that the attributes are at the <span style=\"color:blue\"> same scale (usually between 0 and 1 or -1 and 1).</span>\n",
    "\n",
    "![Example Image](scaling.png)\n",
    "\n",
    "\n",
    "Many ML algorithms require scaled data like **KNN**, **K-means**, **linear regression**, **logistic regression** and **neural networks**. \n",
    "\n",
    "There are many scaling methods like [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), [`MaxAbsScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html), [`RobustScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) and [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "\n",
    "The most used is [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "**Min-max** is a scaling technique where values are rescaled between **0 and 1** or between **-1 and 1**. \n",
    "\n",
    "\n",
    "Given an original value $x_i$ in the dataset, the corresponding scaled value $x_{\\text{scaled}_i}$  is calculated as follows:\n",
    "\n",
    "<div style= background-color:lightblue; padding:10px\">\n",
    "\n",
    "$$\n",
    "x_{\\text{scaled}_i} = \\frac{x_i - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)} \\times (\\text{max}_{\\text{new}} - \\text{min}_{\\text{new}}) + \\text{min}_{\\text{new}}\n",
    "$$\n",
    "</div>\n",
    "Where:\n",
    "\n",
    "- $( x_i $) is the original value.\n",
    "- $( \\text{min}(x)$) and $( \\text{max}(x)$) are the minimum and maximum values of the original dataset, respectively.\n",
    "- $( \\text{min}_{ \\text{new}} $) and $( \\text{max}_{\\text{new}} $) are the minimum and maximum values of the desired range (e.g., 0 and 1 or -1 and 1).\n",
    "\n",
    "Let's illustrate this with an **example**:\n",
    "\n",
    "Suppose you have a dataset with one feature and the original values are [10, 20, 30, 50]. \n",
    "\n",
    "###### 1) Use the above formula to scale the values between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4411f",
   "metadata": {},
   "source": [
    "**a. Compute the minimum and maximum values for the feature**\n",
    "\n",
    "- $ \\text{min}(x) $= 10\n",
    "- $ \\text{max}(x) $= 50\n",
    "- $ \\text{max}_{\\text{new}}$= 1\n",
    "- $ \\text{mix}_{\\text{new}}$= 0\n",
    "\n",
    "**b. Scale each value using the Min-Max Scaling formula**\n",
    "- For 10: $( x_{\\text{scaled}} = \\frac{10 - 10}{50 - 10} \\times (1 - 0) + 0 = \\frac{0}{40} + 0 = 0 $)\n",
    "- For 20: $( x_{\\text{scaled}} = \\frac{20 - 10}{50 - 10} \\times (1 - 0) + 0 = \\frac{10}{40} + 0 = 0.25 $)\n",
    "- For 30: $( x_{\\text{scaled}} = \\frac{40 - 10}{50 - 10} \\times (1 - 0) + 0 = \\frac{30}{40} + 0 = 0.75 $)\n",
    "- For 50: $( x_{\\text{scaled}} = \\frac{50 - 10}{50 - 10} \\times (1 - 0) + 0 = \\frac{40}{40} + 0 = 1 $)\n",
    "\n",
    "The sacaled values are : 0, 0.25, 0.75 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e4ebc",
   "metadata": {},
   "source": [
    "###### 2)  Let's scale the values between -1 and 1 \n",
    "\n",
    "###### 3) Now, let's using Min-Max Scaler from scikit-learn library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da18cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Original dataset : it is a two-dimensional numpy array (https://numpy.org/doc/stable/reference/generated/numpy.array.html)\n",
    "data = [[10], [20], [30],  [50]]\n",
    "\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))  # Set the desired range\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd4352",
   "metadata": {},
   "source": [
    "##### Exercice \n",
    "Now you can download the diabetes dataset (from the dataset folder) and use the data scaling Min-Max if necessary. For more information on this dataset, please visit (https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61da78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c6a54d4",
   "metadata": {},
   "source": [
    "Now, we will use another scaling method: [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) which removes the mean and scaling to unit variance.\n",
    "\n",
    "This method consists in two steps: \n",
    "\n",
    "a) **Centering the Data**:\n",
    "   - For each feature (column) in the dataset, the mean of that feature is calculated.\n",
    "   - Then, the mean is subtracted from each value in the feature column. This process centers the feature distribution around zero.\n",
    "\n",
    "b) **Scaling to Unit Variance**:\n",
    "   - After centering the data, the next step is to scale each feature so that it has a unit variance.\n",
    "   - This is achieved by dividing each value in the feature column by the standard deviation of that feature.\n",
    "\n",
    "Mathematically, the transformation applied to each feature $( x_i $) can be represented as follows:\n",
    "<div style= background-color:lightblue; padding:1px\">\n",
    "$x_{\\text{scaled}_i} = \\frac{x_i - \\text{mean}(x)}{\\text{std}(x)} $\n",
    "</div>\n",
    "                                                  \n",
    "Where:\n",
    "- $( x_{\\text{scaled}_i} $) is the scaled value of the feature $( x_i $).\n",
    "- $( \\text{mean}(x)$) is the mean of the feature $( x_i $).\n",
    "- $( \\text{std}(x) $) is the standard deviation of the feature $( x_i $).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb5efa",
   "metadata": {},
   "source": [
    "Let's illustrate with the same example:\n",
    "\n",
    "a. **Centering the Data**:\n",
    "   - Mean of the dataset: $( \\text{mean}(x) = \\frac{10 + 20 + 30 + 50}{4} = \\frac{110}{4} = 27.5 $)\n",
    "   - Centered data: Subtract the mean from each value in the dataset.\n",
    "   - Centered data: $([10 - 27.5, 20 - 27.5, 30 - 27.5, 50 - 27.5] = [-17.5, -7.5, 2.5, 22.5]$)\n",
    "\n",
    "b. **Scaling to Unit Variance**:\n",
    "   - Standard Deviation of the dataset: $( \\text{std}(x) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\text{mean}(x))^2} = \n",
    "    \\sqrt{\\frac{1}{4} \\sum_{i=1}^{4} (-17.5)^2 + (-7.5)^2 + (2.5)^2 + (22.5)^2} $)\n",
    "   - Standard Deviation of the dataset: $( \\text{std}(x) = \\sqrt{\\frac{1}{4} (306.25 + 56.25 + 6.25 + 506.25)} = \\sqrt{\\frac{875}{4}} = \\sqrt{218.75} \\approx 14.76 $)\n",
    "   - Scaled data: Divide each value in the centered data by the standard deviation.\n",
    "   - Scaled data: $([-17.5 / 14.76, -7.5 / 14.76, 2.5 / 14.76, 22.5 / 14.76] $)\n",
    "\n",
    "Let's calculate the scaled values:\n",
    "\n",
    "- For 10: $( \\frac{-17.5}{14.76} \\approx -1.18 $)\n",
    "- For 20: $( \\frac{-7.5}{14.76} \\approx -0.51 $)\n",
    "- For 30: $( \\frac{2.5}{14.76} \\approx 0.17 $)\n",
    "- For 50: $( \\frac{22.5}{14.76} \\approx 1.52 $)\n",
    "\n",
    "So, the scaled values using StandardScaler for the modified dataset would be  $([-1.18, -0.51, 0.17, 1.52]$).\n",
    "\n",
    "###### Now, let's try it from scikit-learn library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02667bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Original dataset\n",
    "data = [[10], [20], [30],  [50]]\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd85434b",
   "metadata": {},
   "source": [
    "Let's rescaling the diabetes dataset using the standarScaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652e76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204073ea-a9ca-485d-bf24-1914468f451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63994726  0.84832379  0.14964075  0.90726993 -0.69289057  0.20401277\n",
      "   0.46849198  1.4259954   1.36589591]\n",
      " [-0.84488505 -1.12339636 -0.16054575  0.53090156 -0.69289057 -0.68442195\n",
      "  -0.36506078 -0.19067191 -0.73212021]\n",
      " [ 1.23388019  1.94372388 -0.26394125 -1.28821221 -0.69289057 -1.10325546\n",
      "   0.60439732 -0.10558415  1.36589591]\n",
      " [-0.84488505 -0.99820778 -0.16054575  0.15453319  0.12330164 -0.49404308\n",
      "  -0.92076261 -1.04154944 -0.73212021]\n",
      " [-1.14185152  0.5040552  -1.50468724  0.90726993  0.76583594  1.4097456\n",
      "   5.4849091  -0.0204964   1.36589591]\n",
      " [ 0.3429808  -0.15318486  0.25303625 -1.28821221 -0.69289057 -0.81134119\n",
      "  -0.81807858 -0.27575966 -0.73212021]\n",
      " [-0.25095213 -1.34247638 -0.98770975  0.71908574  0.07120427 -0.12597727\n",
      "  -0.676133   -0.61611067  1.36589591]\n",
      " [ 1.82781311 -0.184482   -3.57259724 -1.28821221 -0.69289057  0.41977549\n",
      "  -1.02042653 -0.36084741 -0.73212021]\n",
      " [-0.54791859  2.38188392  0.04624525  1.53455054  4.02192191 -0.18943689\n",
      "  -0.94794368  1.68125866  1.36589591]\n",
      " [ 1.23388019  0.12848945  1.39038675 -1.28821221 -0.69289057 -4.06047387\n",
      "  -0.7244549   1.76634642  1.36589591]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "print(scaled_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7eb7c1a-e4c5-4367-b352-b296b2504ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35294118 0.74371859 0.59016393 0.35353535 0.         0.50074516\n",
      "  0.23441503 0.48333333 1.        ]\n",
      " [0.05882353 0.42713568 0.54098361 0.29292929 0.         0.39642325\n",
      "  0.11656704 0.16666667 0.        ]\n",
      " [0.47058824 0.91959799 0.52459016 0.         0.         0.34724292\n",
      "  0.25362938 0.18333333 1.        ]\n",
      " [0.05882353 0.44723618 0.54098361 0.23232323 0.11111111 0.41877794\n",
      "  0.03800171 0.         0.        ]\n",
      " [0.         0.68844221 0.32786885 0.35353535 0.19858156 0.64232489\n",
      "  0.94363792 0.2        1.        ]\n",
      " [0.29411765 0.58291457 0.60655738 0.         0.         0.38152012\n",
      "  0.05251921 0.15       0.        ]\n",
      " [0.17647059 0.3919598  0.40983607 0.32323232 0.10401891 0.46199702\n",
      "  0.07258753 0.08333333 1.        ]\n",
      " [0.58823529 0.57788945 0.         0.         0.         0.52608048\n",
      "  0.02391119 0.13333333 0.        ]\n",
      " [0.11764706 0.98994975 0.57377049 0.45454545 0.64184397 0.45454545\n",
      "  0.03415884 0.53333333 1.        ]\n",
      " [0.47058824 0.6281407  0.78688525 0.         0.         0.\n",
      "  0.06575576 0.55       1.        ]]\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.352941  0.743719  0.590164  0.353535  0.000000  0.500745  0.234415   \n",
      "1  0.058824  0.427136  0.540984  0.292929  0.000000  0.396423  0.116567   \n",
      "2  0.470588  0.919598  0.524590  0.000000  0.000000  0.347243  0.253629   \n",
      "3  0.058824  0.447236  0.540984  0.232323  0.111111  0.418778  0.038002   \n",
      "4  0.000000  0.688442  0.327869  0.353535  0.198582  0.642325  0.943638   \n",
      "\n",
      "          7    8  \n",
      "0  0.483333  1.0  \n",
      "1  0.166667  0.0  \n",
      "2  0.183333  1.0  \n",
      "3  0.000000  0.0  \n",
      "4  0.200000  1.0  \n"
     ]
    }
   ],
   "source": [
    "#Return a Numpy representation of the DataFrame\n",
    "array = data.values\n",
    "\n",
    "# Initialize MiniMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # Set the desired range \n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(array)\n",
    "\n",
    "\n",
    "print(scaled_data[:10])\n",
    "\n",
    "#transform the numpy to a dataframe\n",
    "df = pd.DataFrame(data=scaled_data)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d752cee",
   "metadata": {},
   "source": [
    "#### 2) Data Normalization \n",
    "\n",
    "In scaling, you're changing the range of your data, while in normalization, you're changing the shape of the distribution of your data. Normalization is used to rescale each row of data. \n",
    "![Example Image](normalization.png)\n",
    "\n",
    "It is mainly useful in sparse dataset where we have a lot of zeros.\n",
    "In machine learning, there are two types of normalization preprocessing techniques : L1 normalization (known as Manhattan normalization) and L2 normalization. \n",
    "\n",
    "###### L1 Normalization (Least Absolute Deviations):\n",
    "For L1 normalization, each component of the vector is divided by the L1 norm of the vector, which is the sum of the absolute values of its components:\n",
    "\n",
    "                                                   L1_norm = |v1| + |v2| + ... + |vn|\n",
    "Then, we divide each component of the vector by the L1 norm:\n",
    "\n",
    "<div style= \"background-color:lightblue; padding:10px\"> \n",
    "                                        \n",
    "Normalized_value_i = value_i / L1_norm\n",
    "</div>\n",
    "\n",
    "Let's have an example : \n",
    "\n",
    "a. we calculate the L1 norm of the vector, which is the sum of the absolute values of its components:\n",
    "     - L1_norm = |10| + |20| + |30| + |50| = 10 + 20 + 30 + 50 = 110\n",
    "b. we divide each component of the vector by the L1 norm:\n",
    "     - Normalized_values_L1 = [10/110, 20/110, 30/110, 50/110] \n",
    "                            = [1/11, 2/11, 3/11, 5/11] \n",
    "                            = [0.0909, 0.1818, 0.2727, 0.4545]\n",
    "     \n",
    "###### L2 Normalization (Least Squares):\n",
    "\n",
    "For L2 normalization, each component of the vector is divided by the L2 norm of the vector, which is the square root of the sum of the squares of its components:\n",
    "\n",
    "- L2_norm = sqrt(v1^2 + v2^2 + ... + vn^2)\n",
    "\n",
    "Then, we divide each component of the vector by the L2 norm:\n",
    "<div style= \"background-color:lightblue; padding:10px\"> \n",
    "Normalized_value_i = value_i / L2_norm\n",
    "</div>\n",
    "\n",
    "Let's have an example : [10, 20, 30, 50]\n",
    "\n",
    "we calculate the L2 norm of the vector, which is the square root of the sum of the squares of its components:\n",
    "- L2_norm = sqrt(10^2 + 20^2 + 30^2 + 50^2) = sqrt(100 + 400 + 900 + 2500) = sqrt(3900) ≈ 62.45\n",
    "- Normalized_values_L2 = [10/62.45, 20/62.45, 30/62.45, 50/62.45]= [0.1602, 0.3205, 0.4807, 0.8012].\n",
    "\n",
    "Let's using the scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "365324c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "[[10 20 30 50]]\n",
      "\n",
      "L1 Normalized Data:\n",
      "[[0.09090909 0.18181818 0.27272727 0.45454545]]\n",
      "\n",
      "L2 Normalized Data:\n",
      "[[0.16012815 0.32025631 0.48038446 0.80064077]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[10, 20, 30, 50]])\n",
    "\n",
    "# Initialize Normalizer for L1 normalization\n",
    "normalizer_L1 = Normalizer(norm='l1') \n",
    "# Initialize Normalizer for L2 normalization\n",
    "normalizer_L2 = Normalizer(norm='l2')  \n",
    "\n",
    "# Apply L1 normalization\n",
    "normalized_data_L1 = normalizer_L1.transform(data)\n",
    "# Apply L2 normalization\n",
    "normalized_data_L2 = normalizer_L2.transform(data)\n",
    "\n",
    "print(\"Data:\")\n",
    "print(data)\n",
    "print(\"\\nL1 Normalized Data:\")\n",
    "print(normalized_data_L1)\n",
    "print(\"\\nL2 Normalized Data:\")\n",
    "print(normalized_data_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640029b",
   "metadata": {},
   "source": [
    "##### Let's normalize the diabetes dataset using the L1 and the L2 normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dba85e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0              6      148             72             35        0  33.6   \n",
      "1              1       85             66             29        0  26.6   \n",
      "2              8      183             64              0        0  23.3   \n",
      "3              1       89             66             23       94  28.1   \n",
      "4              0      137             40             35      168  43.1   \n",
      "..           ...      ...            ...            ...      ...   ...   \n",
      "763           10      101             76             48      180  32.9   \n",
      "764            2      122             70             27        0  36.8   \n",
      "765            5      121             72             23      112  26.2   \n",
      "766            1      126             60              0        0  30.1   \n",
      "767            1       93             70             31        0  30.4   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                       0.627   50        1  \n",
      "1                       0.351   31        0  \n",
      "2                       0.672   32        1  \n",
      "3                       0.167   21        0  \n",
      "4                       2.288   33        1  \n",
      "..                        ...  ...      ...  \n",
      "763                     0.171   63        0  \n",
      "764                     0.340   27        0  \n",
      "765                     0.245   30        0  \n",
      "766                     0.349   47        1  \n",
      "767                     0.315   23        0  \n",
      "\n",
      "[768 rows x 9 columns]\n",
      "\n",
      "L1 Normalized Data:\n",
      "[[0.01732967 0.42746522 0.20795605 ... 0.00181095 0.14441392 0.00288828]\n",
      " [0.00418496 0.35572147 0.27620726 ... 0.00146892 0.12973371 0.        ]\n",
      " [0.02564333 0.5865911  0.20514662 ... 0.00215404 0.10257331 0.00320542]\n",
      " ...\n",
      " [0.01283878 0.31069856 0.18487848 ... 0.0006291  0.0770327  0.        ]\n",
      " [0.0037672  0.47466745 0.22603212 ... 0.00131475 0.17705849 0.0037672 ]\n",
      " [0.00402067 0.37392196 0.28144664 ... 0.00126651 0.09247532 0.        ]]\n",
      "\n",
      "L2 Normalized Data:\n",
      "[[0.03355185 0.82761219 0.40262215 ... 0.00350617 0.27959871 0.00559197]\n",
      " [0.008424   0.71604034 0.55598426 ... 0.00295683 0.26114412 0.        ]\n",
      " [0.04039717 0.9240852  0.32317734 ... 0.00339336 0.16158867 0.00504965]\n",
      " ...\n",
      " [0.02691539 0.65135243 0.38758161 ... 0.00131885 0.16149234 0.        ]\n",
      " [0.00665291 0.83826692 0.39917472 ... 0.00232187 0.31268687 0.00665291]\n",
      " [0.00791454 0.73605211 0.55401772 ... 0.00249308 0.18203439 0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyadm\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but Normalizer was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\zyadm\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but Normalizer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Initialize Normalizer for L1 normalization\n",
    "normalizer_L1 = Normalizer(norm='l1') \n",
    "# Initialize Normalizer for L2 normalization\n",
    "normalizer_L2 = Normalizer(norm='l2')  \n",
    "\n",
    "# Apply L1 normalization\n",
    "normalized_data_L1 = normalizer_L1.transform(data)\n",
    "# Apply L2 normalization\n",
    "normalized_data_L2 = normalizer_L2.transform(data)\n",
    "\n",
    "print(\"Data:\")\n",
    "print(data)\n",
    "print(\"\\nL1 Normalized Data:\")\n",
    "print(normalized_data_L1)\n",
    "print(\"\\nL2 Normalized Data:\")\n",
    "print(normalized_data_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b134e0",
   "metadata": {},
   "source": [
    "#### 3) Data transformation \n",
    "\n",
    "##### 3.1 Label Encoding \n",
    "\n",
    "**Label encoding** is a technique used to transform categories into numerical values, which can be useful for some machine learning algorithms that require numerical inputs.\n",
    "For example, if you have a categorical variable \"Color\" with categories \n",
    "\n",
    "| color   |\n",
    "|------------|\n",
    "| Red        |\n",
    "| Blue       |\n",
    "| Green      |\n",
    "| Red        |\n",
    "| Yellow     |\n",
    "\n",
    "Label Encoding can map these categories to: \n",
    "\n",
    "| Category   | Encoded |\n",
    "|------------|---------|\n",
    "| Red        | 0       |\n",
    "| Blue       | 1       |\n",
    "| Green      | 2       |\n",
    "| Red        | 0       |\n",
    "| Yellow     | 3       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c140fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example data\n",
    "colors = ['Red', 'Blue', 'Green','Red', 'Yellow']\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform data\n",
    "encoded_colors = label_encoder.fit_transform(colors)\n",
    "\n",
    "# Print encoded data\n",
    "print(\"Encoded Colors:\", encoded_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1084709",
   "metadata": {},
   "source": [
    "However, it's **important** to note that Label Encoding can introduce artificial order among the categories, which may be inappropriate especially if the categories are not ordered. Therefore, it's often preferred to use Label Encoding only for **ordered categorical variables** or to code the target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb2517",
   "metadata": {},
   "source": [
    "##### 3.2 One-Hot Encoding:\n",
    "\n",
    "**One-Hot encoding** is a technique used to encode a category in a **binary vector** (so as not to have an order relationship).  \n",
    "For example, if you have the same categorical variable \"Color\" with categories \n",
    "\n",
    "\n",
    "| color   |\n",
    "|------------|\n",
    "| Red        |\n",
    "| Blue       |\n",
    "| Green      |\n",
    "| Red        |\n",
    "| Yellow     |\n",
    "\n",
    "After one-hot encoding, we will have: \n",
    "\n",
    "| Color_Blue | Color_Green | Color_Red | Color_Yellow |\n",
    "|------------|-------------|-----------|--------------|\n",
    "| 0.0        | 0.0         | 1.0       | 0.0          |\n",
    "| 1.0        | 0.0         | 0.0       | 0.0          |\n",
    "| 0.0        | 1.0         | 0.0       | 0.0          |\n",
    "| 0.0        | 0.0         | 1.0       | 0.0          |\n",
    "| 0.0        | 0.0         | 0.0       | 1.0          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8cd437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Example data\n",
    "colors = ['Red', 'Blue', 'Green', 'Red', 'Yellow']\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame({'Color': colors})\n",
    "\n",
    "# Perform One-Hot Encoding\n",
    "one_hot_encoded = pd.get_dummies(df['Color'])\n",
    "\n",
    "# Print One-Hot Encoded data\n",
    "print(\"One-Hot Encoded Data:\")\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5463c42",
   "metadata": {},
   "source": [
    "##### Exercice. Let's encode the class column of the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44ff5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "encoded_class = label_encoder.fit_transform(iris['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb01747",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightblue; padding:1px\">\n",
    "\n",
    "### A summary \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "When to use scaling and when to use normalization depends on the specific requirements of your data and the machine learning algorithm you're using.\n",
    "\n",
    "- **Scaling**: Scaling is typically used when the features in your dataset have different ranges, and you want to bring them to a similar scale. This is often important for algorithms that are sensitive to the scale of the features, such as support vector machines (SVM) or k-nearest neighbors (KNN), logistic regression, K-means. \n",
    "\n",
    "- **Normalization**: Normalization is useful when the distribution of the feature's values is skewed or has outliers. Outliers are data points that deviate from the rest of the data set. Normalization ensures that all features have the same influence on the model. This is especially important for algorithms that use distance measures, such as KNN,  k-means clustering or gradient descent optimization. \n",
    "\n",
    "- **Encoding** is used to transform ordered categorical variables into numerical values, while **One-Hot Encoding** is used to transform unordered categorical variables into binary variables for use in machine learning models.\n",
    "    \n",
    "</div> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
